---
title: "The Report"
author: "Group 6"
date: "2024-05-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(fpp3)
library(grid)
library(gridExtra)
library(forecast)
library(tseries)
library(dplyr)
library(feasts)
library(fabletools)
library(tsibble)
library(ggplot2)
library(latex2exp)
library(nnet)
```

# 1. Exploratory data analysis

The data set contains information about the quarterly (chain volume) gross domestic product (QGDP) for all ANZSIC06 industry groups in New Zealand, measured in the prices from 2009/10 in NZD Millions from 1987 Q2 until 2021 Q4. Our task is exploring the quarterly GDP in Local Government Administration group. 

```{r}
# read-in data
train <- read_csv("qgdp_training.csv",show_col_types = FALSE)
# convert data into tsibble
train <- train %>% mutate(Quarter = yearquarter(Date)) %>%  
  select(Quarter,`Local Government Administration`) %>% 
  as_tsibble(index = Quarter)
```
Firstly, we look at time plot and relevant plots.

```{r}
# time plot
train %>% autoplot(`Local Government Administration`) + ggtitle(" GDP in Local Government Administration") + 
  xlab("Time") + ylab("NZD (Millions)") +
  theme_bw()+ 
  theme(plot.title = element_text(hjust = 0.5)) 
# Seasonal subseries plot with gg_subseries
train %>%
  gg_subseries(`Local Government Administration`) +
  ylab("NZD(Millions)")
# correlogram
train %>%
  ACF(`Local Government Administration`, lag_max = 50) %>%
  autoplot() 
```

Looking at the time plot, we observed that this is a trending with structure break and seasonality time series. More specific, the average GDP for Q4 was the highest value while the average GDP for Q1 was the smallest value based on the sub-series plot. We can also see an increasing year-on-year trend for each quarter in the sub-series plot and the GDP, on average, increases as the year increases. 
Also, we can see the autocorrelations for small lags tend to be large and positive and decays slowly as the lags increases and the seasonal pattern is not clear in the correlogram. Also, noticing the reverse pattern appeared after lag 43, suggesting the structure break is in this trended time series. 

Next, we have a look at the decomposition plot by using STL decomposition. We decide to use STL model because it allows the seasonal component change over time and we see the magnitude of variation around the trend-cycle does not really vary with the level, so we only consider additive decomposition.   

```{r}
# decomposition
stl.dcmp <- train %>%
  model(STL(`Local Government Administration`,robust = TRUE)) %>%
  components() 
stl.dcmp%>% 
  autoplot()
```

Based on the decomposition result, we can see the trend component dominates the time series, accounting for most of the variability. This is followed by the remainder component, and then the seasonal component. There is a period of time the trend is negative and then stable (from 1990 Q1 until around year of 2003). However, the long-term trend appears to be positive, indicating the GDP in Local Government Administration are increasing. The remainder component appears random, with no discernible patterns, and may be consistent with white noise but we would need further analysis test this. The seasonal component changes quickly over time, where the magnitudes are significantly increasing.



# 2. ETS models


Based on the analysis in the first step, we can confirm that there are indeed trends and seasonal changes, but I am currently unable to determine whether to use multiplicative seasonality or additive seasonality to fit the ets model. And it is necessary to consider whether to adopt a pounded trend. So, I decided to manually try out the pros and cons of these four models first.

```{r}
data <- read_csv("qgdp_training.csv",show_col_types = FALSE)

data <- data %>% mutate(Quarter = yearquarter(Date)) %>%  
  select(Quarter,`Local Government Administration`) %>% 
  as_tsibble(index = Quarter)

fit <- data %>%
       model(
         additive_season = ETS(`Local Government Administration` ~ error("A") + trend("A") + season("A")),
         multiplicative_season = ETS(`Local Government Administration` ~ error("M") + trend("A") + season("M")),
         additive_season_damped = ETS(`Local Government Administration` ~ error("A") + trend("Ad") + season("A")),
         multiplicative_season_damped = ETS(`Local Government Administration` ~ error("M") + trend("Ad") + season("M"))
       )

fc <- fit %>% forecast(h = "5 years")
fc %>% autoplot(data)

# Check model summary
report(fit)

# Use the method of automatically selecting the best model to check if we have selected it correctly.
fit_best <- data %>%
  model(ETS(`Local Government Administration`))
report(fit_best)
```

We can clearly see here that among many aspects of error data, the method of adding seasonality and with Damped trend has the smallest errors, and my method of automatically obtaining the minimum AIC also told me that I should choose the method of adding seasonality and with Damped trend.

## ETS Model Introduction

The ETS (Error, Trend, Seasonality) model we selected is specified as an additive error, damped additive trend, and additive seasonality model. Below are the equations representing this model setup:

### Model Equations

The ETS model equations for an additive error, damped additive trend, and additive seasonality are as follows:

- **Forecast equation:**
  \[
  \hat{y}_{t|t-1} = (l_{t-1} + \phi b_{t-1}) + s_{t-m}
  \]

- **Level equation:**
  \[
  l_t = l_{t-1} + \phi b_{t-1} + \alpha e_t
  \]

- **Trend equation:**
  \[
  b_t = \phi b_{t-1} + \beta e_t
  \]

- **Seasonal equation:**
  \[
  s_t = s_{t-m} + \gamma e_t
  \]

Where:
- \( \hat{y}_{t|t-1} \) is the one-step ahead forecast,
- \( l_t \) is the level component at time \( t \),
- \( b_t \) is the trend component at time \( t \),
- \( s_t \) is the seasonal component at time \( t \),
- \( \phi \) is the damping factor,
- \( \alpha, \beta, \gamma \) are the smoothing parameters,
- \( e_t \) is the forecast error at time \( t \).



# 3. ARIMA models

To fit an appropriate ARIMA model, we first need to test the stationary of the time series data. Because if the time series data is stationary, it means that its statistical characteristics are constant over time, allowing for better modeling and prediction. But here we first need to convert the Date column to date format, as it is quarterly data. And at the same time, it is necessary to convert the data into tsible format. Then, we first conducted KPSS test on the data to check the stationary.

```{r}
# Load the data.
data <- read.csv("qgdp_training.csv")

# Convert the Date column to date format.
data <- data %>%
  mutate(Date = yearquarter(Date))

# Convert the data to tsible format.
data_tsibble <- data %>%
  as_tsibble(index = Date)

# Check the stationary of the original data.
kpss_results <- data_tsibble %>%
  features(Local.Government.Administration, unitroot_kpss)

print(kpss_results)
```

From the test results, according to the KPSS test, the kpss_pvalue is very small, far below the commonly used significance level(0.05), so we reject the null hypothesis that the data is stationary. This means that the data is non-stationary. Therefore, we need to perform a first difference on the data.

```{r}
# Perform a first difference on the data.
data_tsibble <- data_tsibble %>%
  mutate(diff_Local.Government.Administration = difference(Local.Government.Administration))

# Do the KPSS test.
kpss_results_diff <- data_tsibble %>%
  features(diff_Local.Government.Administration, unitroot_kpss)
print(kpss_results_diff)
```

After performing a first difference on the data, we conducted a KPSS test again and found that kpss_pvalue was still below the significance level(0.05). So we still reject the null hypothesis that the data is stationary. This means that the data is still non-stationary. Therefore, we need to perform a second-order difference on the data.

```{r}
# Perform a second-order difference on the data.
data_tsibble <- data_tsibble %>%
  mutate(diff2_Local.Government.Administration = difference(diff_Local.Government.Administration))

# Do the KPSS test.
kpss_results_diff2 <- data_tsibble %>%
  features(diff2_Local.Government.Administration, unitroot_kpss)
print(kpss_results_diff2)
```

From the results, we can see that the kpss_pvalue of the data after the second-order difference is greater than the significance level(0.05). So it means that the data has become stable, so I decided to start fitting the ARIMA model. Firstly, manual fitting is performed. In order to determine the p-value and q-value of the model, we first draw the ACF and PACF plots of the data after the second-order difference on the data.

```{r}
# Draw the ACF plot and PACF plot with the second-order difference data.
gg_tsdisplay(data_tsibble, diff2_Local.Government.Administration, plot_type = "partial")
```

Through the ACF plot, we can locate the p-value candidate values as 2, 3 or 4. Through the PACF plot, we can locate the q-value candidate values as 1 or 3. Because the autocorrelation coefficients are significant at these lagging points. Next, we create a shortlist of appropriate candidate ARIMA models by arranging and combining different q and p values.

```{r}
# Fit the ARIMA model and store them in the shortlist.
models <- data_tsibble %>%
  model(
    arima_220 = ARIMA(Local.Government.Administration ~ pdq(2, 2, 0)),
    arima_221 = ARIMA(Local.Government.Administration ~ pdq(2, 2, 1)),
    arima_021 = ARIMA(Local.Government.Administration ~ pdq(0, 2, 1)),
    arima_023 = ARIMA(Local.Government.Administration ~ pdq(0, 2, 3)),
    arima_211 = ARIMA(Local.Government.Administration ~ pdq(2, 1, 1)),
    arima_420 = ARIMA(Local.Government.Administration ~ pdq(4, 2, 0)),
    arima_321 = ARIMA(Local.Government.Administration ~ pdq(3, 2, 1)),
    arima_320 = ARIMA(Local.Government.Administration ~ pdq(3, 2, 0)),
    stepwise = ARIMA(Local.Government.Administration),
    search = ARIMA(Local.Government.Administration, stepwise = FALSE)
  )

# Report the summary of the models.
glance(models) %>% arrange(AICc)
```

So the candidate models are: (2, 1, 1), (4, 2, 0), (3, 2, 0), (3, 2, 1), (2, 2, 1), (2, 2, 0), (0, 2, 3), (0, 2, 1), and automatic model. By observing the AIC, AICc, and BIC of these models, we can observe that the search model can minimize these parameters. Therefore, we believe that this model is the best model with the best predictive ability. It performs best in balancing goodness of fit and model complexity.

```{r}
# So we think search model is the best model.
best_model <- models %>%
  select(search)
report(best_model)
```

Based on the results of search model, we can write the fitted model equation in backshift notation:

\documentclass{article}
\usepackage{amsmath}
\begin{document}

\[
(1 - 0.0764B - 0.2787B^2) (1 - B^4) \nabla y_t = (1 - 0.6220B^4) \epsilon_t
\]

\end{document}

# 4. Neural network autoregression (NNAR) models

Neural Network Autoregression (NNAR) model is a type of forecasting model that combines the principles of neural networks and autoregressive (AR) models. NNAR models are useful in time series forecasting, where past values of a series are used to predict future values.

Neural networks consist layers of interconnected nodes (neurons) that learn complex patterns in data through training. They are powerful for modelling non-linear relationships and also allow complex non-linear relationships between the response variable and its predictors.

The NNAR model combines these two approaches, which uses previous time series values (like the AR model) as input features for a neural network. The neural network then processes the inputs to predict future values.

For time series data, lagged values of the time series can be used as inputs to a neural network. This is the Neural Network Autoregression (NNAR) model.

Structure of the NNAR model: 
1. **Input Layer**: Receives *p* lagged values of the time series (similar to AR model inputs).
2. **Hidden Layers**: One or more hidden layers , with neurons that apply non-linear transformations to the inputs. The number of neurons and hidden layers can vary based on the complexity required.
3. **Output Layer**: Typically a single neuron that outputs the predicted future value.

The notation NNAR(*p*, *k*) indicates there are *p* lagged inputs and *k* nodes in the hidden layer. For example, a NNAR(8,4) model is a neural network with the last eight observations ($y_{t-1} ,  y_{t-2} , \cdots , y_{t-8}$) used as inputs for forecasting the output $y_{t}$, and has four neurons in the hidden layer.

A NNAR(*p*,0) model is equivalent to an ARIMA(*p*,0,0) model, in terms of using *p* past observations for forecasting. But, note that there are no restrictions on the parameters to ensure stationary.

With seasonal data, we can also add the last observed values from the same season as inputs. NNAR(*p*, *P*, *k*)m model has inputs ($y_{t-1} ,  y_{t-2} , \cdots , y_{t-p} , y_{t-m} , y_{t-2m} , \cdots , y_{t-Pm}$) and *k* neurons in the hidden layer. *P* is the number of lagged seasonal observations used as inputs.

In R, the **NNETAR()** function fits an NNAR(*p*, *P*, *k*)m model. If the values of *p* and *P* are not specified, these will be selected automatically. The values selected is the optimal values. 

For seasonal time series, the default values are *P = 1* and *p* is chosen from the optimal linear model fitted to the seasonally adjusted data. If *k* is not specified, it is set to *k = (p + P + 1)/2* (round to the nearest integer).

```{r}
nnar_model <- train %>% model(NNETAR(`Local Government Administration`))
report(nnar_model)
```

We will use the NNETAR() function. Since we are looking to fit an automatic NNAR model to our dataset, we do not manually specify the values for the parameters p (non-seasonal lags), P (seasonal lags), and k (number of neurons). This is because the NNETAR() function automatically selects the optimal values for the parameters based on the characteristics of the time series data.   

From the output, `Model:  NNAR(1,1,2)[4]` , this takes the format NNAR(*p*, *P*, *k*)m. This indicates that there is 1 lagged observation used as input (autoregressive term)- *p* . There is 1 lagged seasonal observation used as input - *P*. There are 2 nodes in the hidden layer - *k*. The number 4 means quarterly data. *m* is the seasonal frequency. 

The numbers (2-2-1) indicates each network has the structure of 2 input nodes, 2 hidden nodes, and 1 output node. The 2 input nodes correspond to the number of autoregressive and seasonal terms (*p*+ *P*). 2 hidden nodes and 1 output node that produce the model's prediction.


# 5. Assumption checking

```{r}

```


# 6. Forecasting

```{r}

```


# 7. Member contributions

